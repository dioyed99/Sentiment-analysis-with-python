{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "from kafka import KafkaProducer\n",
    "from confluent_kafka import Producer\n",
    "from confluent_kafka import Consumer\n",
    "# import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka configuration\n",
    "bootstrap_servers = \"localhost:9092\"\n",
    "kafka_topic = \"reddit\"\n",
    "producer = Producer({'bootstrap.servers': bootstrap_servers})\n",
    "# producer = Producer(bootstrap_servers=bootstrap_servers, request_timeout_ms=1200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer\n",
    "\n",
    "# Configuration du consommateur\n",
    "config = {'bootstrap.servers': bootstrap_servers, 'group.id': 'Xmen', 'auto.offset.reset': 'earliest'}\n",
    "\n",
    "# Création du consommateur\n",
    "consumer = Consumer(config)\n",
    "\n",
    "# S'abonner à un ou plusieurs topics\n",
    "topics = ['reddit_postgres']\n",
    "consumer.subscribe(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(string, stopwords):\n",
    "    clean_string = re.sub(r'[^\\w\\s]', '', string)\n",
    "    clean_strings = clean_string.lower().split(\" \")\n",
    "    cleans = [x for x in clean_strings if not x in stopwords]\n",
    "    cleaned_string = \" \".join(cleans)\n",
    "    return cleaned_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = stopwords.words('english')\n",
    "stops.extend([\"wa\", \"hi\", \"a\", \"thi\", \"would\", \"had\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', \"could\", \"thus\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "model = joblib.load(\"pipe.pkl\")\n",
    "\n",
    "# SQL statement to create the \"tweets\" table\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS tweets (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    tweet_text TEXT,\n",
    "    tweet_date TIMESTAMP,\n",
    "    topic TEXT,\n",
    "    label TEXT,\n",
    "    clean TEXT\n",
    ");\n",
    "\"\"\"\n",
    "# Execute the table creation query\n",
    "postgres_cursor.execute('DROP TABLE tweets')\n",
    "postgres_cursor.execute(create_table_query)\n",
    "postgres_conn.commit()\n",
    "\n",
    "# Consommer les messages\n",
    "while True:\n",
    "    message = consumer.poll(1.0)  # Attendre pendant 1 seconde au maximum\n",
    "    \n",
    "    if message is None:\n",
    "        continue\n",
    "    \n",
    "    if message.error():\n",
    "        print(f\"Erreur lors de la consommation : {message.error()}\")\n",
    "        continue\n",
    "    \n",
    "    # Traitement du message\n",
    "    print(\"Message reçu : {}\".format(message.value().decode('utf-8')))\n",
    "    \n",
    "\n",
    "    tweet = eval(message.value().decode('utf-8'))\n",
    "\n",
    "    tweet[\"label\"] = model.predict([tweet[\"value\"]])[0]\n",
    "    tweet[\"clean\"] = clean_string(tweet[\"value\"], stops)\n",
    "    # Configure PostgreSQL connection\n",
    "    postgres_connection_params = {\n",
    "        \"host\": 'localhost',\n",
    "        # \"port\": postgres_port,\n",
    "        \"database\": 'reddit',\n",
    "        \"user\": 'postgres',\n",
    "        \"password\": 'elodie98',\n",
    "    }\n",
    "    postgres_conn = psycopg2.connect(**postgres_connection_params)\n",
    "    postgres_cursor = postgres_conn.cursor()\n",
    "\n",
    "    tweet_text = tweet[\"value\"].encode('utf-8', errors='ignore').decode('utf-8')\n",
    "    tweet_date = tweet[\"date\"]\n",
    "    topic = tweet[\"topic\"]\n",
    "    label = tweet[\"label\"]\n",
    "    clean = tweet[\"clean\"]\n",
    "    \n",
    "    postgres_insert_query = \"INSERT INTO tweets (tweet_text, tweet_date, topic, label, clean) VALUES (%s,%s,%s,%s,%s)\"\n",
    "    record_to_insert = (tweet_text, tweet_date, topic, label, clean)\n",
    "    print(record_to_insert)\n",
    "    postgres_cursor.execute(postgres_insert_query, record_to_insert)\n",
    "    postgres_conn.commit()\n",
    "    # Close the PostgreSQL connection\n",
    "    postgres_cursor.close()\n",
    "    postgres_conn.close()\n",
    "    \n",
    "# Fermeture du consommateur\n",
    "consumer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
